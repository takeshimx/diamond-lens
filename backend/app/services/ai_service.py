from typing import Optional, List, Dict, Any
# from google.cloud import bigquery
# from google.oauth2 import service_account
from google.cloud.exceptions import GoogleCloudError
import pandas as pd
import os
import json
import requests
import re
from dotenv import load_dotenv
# from functools import lru_cache
from datetime import datetime
from .bigquery_service import client
import logging
from .conversation_service import get_conversation_service
from .analytics.base_engine import BaseEngine
from backend.app.config.prompt_registry import get_prompt, get_prompt_version

# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ã¨æœ¬ç•ªå®Ÿè¡Œæ™‚ã®ä¸¡æ–¹ã«å¯¾å¿œ
try:
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ã®ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from app.config.query_maps import (
        QUERY_TYPE_CONFIG,
        METRIC_MAP,
        DECIMAL_FORMAT_COLUMNS,
        MAIN_PITCHING_STATS,
        MAIN_BATTING_STATS,
        MAIN_CAREER_BATTING_STATS,
        MAIN_RISP_BATTING_STATS, MAIN_BASES_LOADED_BATTING_STATS, MAIN_RUNNER_ON_1B_BATTING_STATS,
        MAIN_INNING_BATTING_STATS, MAIN_BATTING_BY_PITCHING_THROWS_STATS, MAIN_BATTING_BY_PITCH_TYPE_STATS,
        MAIN_BATTING_BY_GAME_SCORE_SITUATIONS_STATS
    )
    from app.config.statcast_query import KEY_METRICS_QUERY_SELECT
except ImportError:
    # æœ¬ç•ªå®Ÿè¡Œæ™‚ã®çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from backend.app.config.query_maps import (
        QUERY_TYPE_CONFIG,
        METRIC_MAP,
        DECIMAL_FORMAT_COLUMNS,
        MAIN_PITCHING_STATS,
        MAIN_BATTING_STATS,
        MAIN_CAREER_BATTING_STATS,
        MAIN_RISP_BATTING_STATS, MAIN_BASES_LOADED_BATTING_STATS, MAIN_RUNNER_ON_1B_BATTING_STATS,
        MAIN_INNING_BATTING_STATS, MAIN_BATTING_BY_PITCHING_THROWS_STATS, MAIN_BATTING_BY_PITCH_TYPE_STATS,
        MAIN_BATTING_BY_GAME_SCORE_SITUATIONS_STATS
    )
    from backend.app.config.statcast_query import KEY_METRICS_QUERY_SELECT
# from .simple_chart_service import enhance_response_with_simple_chart, should_show_simple_chart # For Development, add backend. path

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.getLogger().handlers = []
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

dotenv_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', '..', '.env')
load_dotenv(dotenv_path=dotenv_path)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€
PROJECT_ID = os.getenv("GCP_PROJECT_ID")
DATASET_ID = os.getenv("BIGQUERY_DATASET_ID")
BATTING_STATS_TABLE_ID = os.getenv("BIGQUERY_BATTING_STATS_TABLE_ID", "fact_batting_stats_with_risp")
PITCHING_STATS_TABLE_ID = os.getenv("BIGQUERY_PITCHING_STATS_TABLE_ID", "fact_pitching_stats")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY_V2")

# Manage Google cloud alient with singleton pattern
SERVICE_ACCOUNT_KEY_PATH = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")


def _parse_query_with_llm(query: str, season: Optional[int]) -> Optional[Dict[str, Any]]:
    """
    [ã‚¹ãƒ†ãƒƒãƒ—1] LLMã‚’ä½¿ã„ã€è³ªå•ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã™ã€‚ã“ã®é–¢æ•°ã¯LLMã®å½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’è§£æã—ã€ã€Œæ„å›³ã€ã‚’æ±²ã¿å–ã‚Šã€
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ¤œç´¢ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§æŠ½å‡ºã—ã¾ã™ã€‚
    """
    if not GEMINI_API_KEY:
        logger.error("GEMINI_API_KEY_V2 is not set.")
        return None

    # Get the prompt version-managed by prompt_registry.py
    prompt = get_prompt(
        "parse_query",
        query=query,
        season=season if season else "None"
    )
    prompt_version = get_prompt_version("parse_query")
    logger.info(f"Using parse_query prompt version: {prompt_version}")

    GEMINI_API_URL=f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}"
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"responseMimeType": "application/json"}
    }

    try:
        response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(payload))
        response.raise_for_status()
        result = response.json()
        if result.get("candidates"):
            json_string = result["candidates"][0]["content"]["parts"][0]["text"]
            params = json.loads(json_string)

            logger.info(f"Parsed parameters: {params}")

            if season and 'season' not in params:
                params['season'] = season
            return params
        return None
    except (requests.exceptions.RequestException, json.JSONDecodeError) as e:
        logger.error(f"Error during LLM query parsing: {e}", exc_info=True)
        return None


def _validate_query_params(params: Dict[str, Any]) -> bool:
    """
    LLMå‡ºåŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼ã‚’è¡Œã„ã¾ã™ã€‚
    SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æ”»æ’ƒã‚„ãã®ä»–ã®ä¸æ­£ãªå…¥åŠ›ã‚’æ¤œå‡ºã—ã¾ã™ã€‚

    Args:
        params: LLMã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
        bool: æ¤œè¨¼ã«åˆæ ¼ã—ãŸå ´åˆTrueã€ä¸æ­£ãªå…¥åŠ›ã‚’æ¤œå‡ºã—ãŸå ´åˆFalse

    æ¤œè¨¼é …ç›®:
        1. é¸æ‰‹å: è‹±å­—ã€ã‚¹ãƒšãƒ¼ã‚¹ã€ãƒ”ãƒªã‚ªãƒ‰ã€ãƒã‚¤ãƒ•ãƒ³ã€ã‚¢ãƒã‚¹ãƒˆãƒ­ãƒ•ã‚£ã®ã¿è¨±å¯
        2. Season: å¦¥å½“ãªå¹´ã®ç¯„å›²ï¼ˆ1900-2100ï¼‰
        3. query_type: ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹å€¤ã®ã¿
        4. split_type: ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹å€¤ã®ã¿
        5. metrics: METRIC_MAPã«å«ã¾ã‚Œã‚‹å€¤ã®ã¿ï¼ˆmain_statsã‚’é™¤ãï¼‰
        6. order_by: METRIC_MAPã«å«ã¾ã‚Œã‚‹å€¤ã®ã¿
        7. pitcher_throws: RHPã¾ãŸã¯LHPã®ã¿
        8. inning: 1-9ã®æ•´æ•°ã®ã¿
        9. strikes/balls: 0-3ã®æ•´æ•°ã®ã¿
        10. é•·ã•åˆ¶é™: ç•°å¸¸ã«é•·ã„æ–‡å­—åˆ—ã‚’æ‹’å¦
    """

    return BaseEngine.validate_query_params(params)


# Helper function to determine query strategy (using a simple query or more complex one)
def _determine_query_strategy(params: Dict[str, Any]) -> str:
    """
    ã‚¯ã‚¨ãƒªã®è¤‡é›‘ã•ã«åŸºã¥ã„ã¦æˆ¦ç•¥ã‚’æ±ºå®šã™ã‚‹
    - è¤‡åˆæ¡ä»¶ãŒ2ã¤ä»¥ä¸Š: statcast master table
    - å˜ä¸€æ¡ä»¶: aggregated table
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ãŒå¿…è¦ãªå ´åˆã®ç‰¹åˆ¥å‡¦ç†ã‚‚å«ã‚€
    """

    return BaseEngine.determine_query_strategy(params)


# Helper function to build dynamic SQL queries with statcast_master_table
def _build_dynamic_statcast_sql(params: Dict[str, Any]) -> tuple[str, dict]:
    """
    statcast_master_tableã«å¯¾ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

    Args:
        params: LLMã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
        tuple[str, dict]: (SQLæ–‡å­—åˆ—, ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸)
    """

    return BaseEngine.build_dynamic_statcast_sql(params)
        
 

# Helper function to build dynamic SQL queries with aggregated table
def _build_dynamic_sql(params: Dict[str, Any]) -> tuple[str, dict]:
    """
    [ã‚¹ãƒ†ãƒƒãƒ—2] æŠ½å‡ºã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…ƒã«ã€BigQueryç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

    Args:
        params: LLMã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
        tuple[str, dict]: (SQLæ–‡å­—åˆ—, ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸)
        - SQLæ–‡å­—åˆ—: ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆ@param_nameï¼‰ã‚’å«ã‚€ã‚¯ã‚¨ãƒª
        - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸: ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã«å¯¾å¿œã™ã‚‹å®Ÿéš›ã®å€¤
    """

    return BaseEngine.build_dynamic_sql(params)


def _generate_final_response_with_llm(original_query: str, data_df: pd.DataFrame) -> str:
    """
    [ã‚¹ãƒ†ãƒƒãƒ—4] å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã¨å…ƒã®è³ªå•ã«åŸºã¥ã„ã¦ã€LLMãŒè‡ªç„¶è¨€èªã®å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    * ã‚¹ãƒ†ãƒƒãƒ—3ã¯BigQueryã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã“ã¨ã§ã™ã€‚
    """
   
    return BaseEngine.generate_final_response_with_llm(original_query, data_df)


def get_ai_response_for_qna_enhanced(
        query: str, 
        season: Optional[int] = None,
        session_id: Optional[str] = None # Id from frontend to track user session
    ) -> Optional[Dict[str, Any]]:
    """
    ã€æ‰“æ’ƒãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ç‰¹åŒ–ç‰ˆã€‘
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®"æ‰“æ’ƒãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰"ã«é–¢ã™ã‚‹è³ªå•ã‚’å‡¦ç†ã—ã¾ã™ã€‚
    """

    # Step 0: Resolve conversation context (ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è§£æ±º == ç›´å‰ã®å±¥æ­´ã‹ã‚‰æƒ…å ±ã‚’è£œå®Œ)
    conv_service = get_conversation_service()
    resolved_query = query
    context_used = False

    if session_id:
        logger.info(f"Resolving conversation context for session_id: {session_id}")
        context_result = conv_service.resolve_context(query, session_id)
        resolved_query = context_result["resolved_query"]
        context_used = context_result["context_used"]

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ season ã‚’è£œå®Œ
        if not season and context_result.get("season"):
            season = int(context_result["season"])
            logger.info(f"Season {season} inferred from conversation context")
        
        if context_used:
            logger.info(f"Query resolved: '{query}' â†’ '{resolved_query}'")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¼šè©±å±¥æ­´ã«ä¿å­˜ã€‚æ¬¡ã®è³ªå•ã§ã€Œã•ã£ãã®è³ªå•ã‚’ã‚‚ã†ä¸€åº¦ã€ãªã©ã®æ–‡è„ˆãŒä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
        conv_service.add_message(session_id, "user", query)

    # Step 1: LLMã§è³ªå•ã‚’è§£æï¼ˆè§£æ±ºå¾Œã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ï¼‰
    query_params = _parse_query_with_llm(resolved_query, season)
    if not query_params:
        logger.warning("Could not extract parameters from the query.")
        error_response = {
            "answer": "è³ªå•ã‚’ç†è§£ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ‰“æ’ƒæˆç¸¾ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„ã€‚ï¼ˆä¾‹ï¼š2024å¹´ã®ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³ç‹ã¯èª°ï¼Ÿï¼‰",
            "isTable": False
        }

        # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚‚ä¼šè©±å±¥æ­´ã«ä¿å­˜
        if session_id:
            conv_service.add_message(session_id, "assistant", error_response["answer"])
        
        return error_response
    
    logger.info(f"Parsed query parameters: {query_params}")

    # Step 1.5: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼ï¼ˆSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰
    if not _validate_query_params(query_params):
        logger.error(f"Security validation failed for parameters: {query_params}")
        return {
            "answer": "ä¸æ­£ãªå…¥åŠ›ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚æ­£ã—ã„å½¢å¼ã§è³ªå•ã—ã¦ãã ã•ã„ã€‚",
            "isTable": False
        }

    # Step 2: Build SQL with parameterization
    query_strategy = _determine_query_strategy(query_params)
    logger.info(f"Using query strategy: {query_strategy}")

    if query_strategy == "aggregated_table":
        # Using aggregated table
        sql_query, sql_parameters = _build_dynamic_sql(query_params)
        if not sql_query:
            logger.warning("Failed to build SQL query.")
            return {
                "answer": "ã“ã®è³ªå•ã«å¯¾å¿œã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
                "isTable": False
            }
        logger.info(f"Generated parameterized SQL query:\n{sql_query}")
        logger.info(f"Query parameters: {sql_parameters}")

    else: # Using statcast master table
        sql_query, sql_parameters = _build_dynamic_statcast_sql(query_params)
        if not sql_query:
            logger.warning("Failed to build SQL query with statcast master table.")
            return {
                "answer": "ã“ã®è³ªå•ã«å¯¾å¿œã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
                "isTable": False
            }
        logger.info(f"Generated parameterized SQL query (strategy: {query_strategy}):\n{sql_query}")
        logger.info(f"Query parameters: {sql_parameters}")

    # Step 3: Fetch data from BigQuery with parameterized query
    try:
        from google.cloud.bigquery import QueryJobConfig, ScalarQueryParameter, ArrayQueryParameter

        # BigQueryç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã‚’ä½œæˆ
        query_parameters_list = []

        for key, value in sql_parameters.items():
            if isinstance(value, list):
                # é…åˆ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¾‹: pitch_types, inningsï¼‰
                # é…åˆ—ã®è¦ç´ ã®å‹ã‚’åˆ¤å®š
                if value and isinstance(value[0], int):
                    param = ArrayQueryParameter(key, "INT64", value)
                    query_parameters_list.append(param)
                    logger.debug(f"Added array parameter: {key} = {value} (INT64)")
                else:
                    param = ArrayQueryParameter(key, "STRING", value)
                    query_parameters_list.append(param)
                    logger.debug(f"Added array parameter: {key} = {value} (STRING)")
            elif isinstance(value, int):
                # æ•´æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¾‹: season, inning, limitï¼‰
                param = ScalarQueryParameter(key, "INT64", value)
                query_parameters_list.append(param)
                logger.debug(f"Added scalar parameter: {key} = {value} (INT64)")
            else:
                # æ–‡å­—åˆ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¾‹: player_name, pitcher_throwsï¼‰
                param = ScalarQueryParameter(key, "STRING", str(value))
                query_parameters_list.append(param)
                logger.debug(f"Added scalar parameter: {key} = {value} (STRING)")

        job_config = QueryJobConfig(query_parameters=query_parameters_list)

        logger.info(f"Total query parameters configured: {len(query_parameters_list)}")
        logger.debug(f"Query parameters list: {[p.name for p in query_parameters_list]}")

        query_start = datetime.now()
        results_df = client.query(sql_query, job_config=job_config).to_dataframe()
        query_duration = (datetime.now() - query_start).total_seconds()

        logger.info(f"Query completed in {query_duration:.2f}s, fetched {len(results_df)} rows")

        # Performance warning for slow queries
        if query_duration > 10:  # 10ç§’ä»¥ä¸Š
            logger.warning(f"Slow query detected: {query_duration:.2f}s")
    except GoogleCloudError as e:
        logger.error(f"BigQuery query failed: {e}", exc_info=True)

        # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        error_message = "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
        if "timeout" in str(e).lower():
            error_message += "ã‚¯ã‚¨ãƒªãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚æ¡ä»¶ã‚’çµã£ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
        elif "quota" in str(e).lower():
            error_message += "åˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
        
        return {
            "answer": error_message,
            "isTable": False
        }
    
    if results_df.empty:
        return {
            "answer": "æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’å¤‰æ›´ã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚",
            "isTable": False
        }
    
    # Step 4: Format response
    total_duration = (datetime.now() - query_start).total_seconds()
    logger.info(f"Total request processing time: {total_duration:.2f}s")
    
    # if output format is table
    if query_params.get("output_format") == "table":
        # Debug logging
        logger.info(f"DataFrame columns: {results_df.columns.tolist()}")
        logger.info(f"DataFrame dtypes: {results_df.dtypes.to_dict()}")
        logger.info(f"First row sample: {results_df.iloc[0].to_dict() if len(results_df) > 0 else 'No data'}")
        
        # Use centralized decimal columns configuration
        decimal_columns = DECIMAL_FORMAT_COLUMNS
        
        # Force decimal columns to have proper numeric types BEFORE converting to dict
        for col in decimal_columns:
            if col in results_df.columns:
                # Convert to numeric, coercing errors to NaN, then fill NaN with None
                results_df[col] = pd.to_numeric(results_df[col], errors='coerce')
                results_df[col] = results_df[col].where(pd.notnull(results_df[col]), None)
        
        # Convert to dictionary
        table_data = results_df.to_dict('records')
        
        # Post-process to ensure decimal formatting
        for row in table_data:
            for col in decimal_columns:
                if col in row and row[col] is not None:
                    try:
                        # Ensure it's a proper decimal number
                        value = float(row[col])
                        if not pd.isna(value):
                            row[col] = round(value, 3)
                        else:
                            row[col] = None
                    except (ValueError, TypeError) as e:
                        logger.warning(f"Could not convert {col} value {row[col]} to float: {e}")
                        # Keep original value
                        pass
        
        # Debug the final table data
        logger.info(f"Final table_data sample: {table_data[0] if table_data else 'No data'}")
        
        columns = [{"key": col, "label": col.replace('_', ' ').title()} for col in results_df.columns]
        
        # Check if single row result for transposition
        is_single_row = len(results_df) == 1
        
        # Add grouping metadata for career batting
        grouping_info = None
        if query_params.get("query_type") == "career_batting":
            # Get base info columns (name, team, etc.)
            base_columns = [col for col in results_df.columns if col in ['name', 'batter_name', 'career_last_team']]
            career_base_columns = [col for col in results_df.columns if col.startswith('career_') and '_at_' not in col and '_by_' not in col]
            risp_columns = [col for col in results_df.columns if '_at_risp' in col]
            bases_loaded_columns = [col for col in results_df.columns if '_at_bases_loaded' in col]
            
            grouping_info = {
                "type": "career_batting_chunks",
                "groups": [
                    {
                        "name": "Career Stats",
                        "columns": base_columns + career_base_columns
                    },
                    {
                        "name": "Career RISP Stats", 
                        "columns": risp_columns
                    },
                    {
                        "name": "Career Bases Loaded Stats",
                        "columns": bases_loaded_columns
                    }
                ]
            }
        
        table_response = {
            "answer": f"ä»¥ä¸‹ã¯{len(results_df)}ä»¶ã®çµæœã§ã™ï¼š",
            "isTable": True,
            "isTransposed": is_single_row,
            "tableData": table_data,
            "columns": columns,
            "decimalColumns": [col for col in results_df.columns if col in DECIMAL_FORMAT_COLUMNS],
            "grouping": grouping_info
        }

        # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºã‚‚å±¥æ­´ã«ä¿å­˜
        if session_id:
            # ãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“ã§ã¯ãªãè¦ç´„ã‚’ä¿å­˜ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ï¼‰
            summary = f"{len(results_df)}ä»¶ã®{query_params.get('query_type', 'çµæœ')}ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"
            conv_service.add_message(
                session_id,
                "assistant",
                summary,
                metadata={ # å¾Œã§åˆ†æã«ä½¿ãˆã‚‹ï¼ˆä¾‹: ã©ã®é¸æ‰‹ãŒã‚ˆãæ¤œç´¢ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰
                    "query_type": query_params.get("query_type"),
                    "player_name": query_params.get("name"),
                    "is_table": True,
                    "context_used": context_used
                }
            )
        
        return table_response

    # Step 4: Generate final response with LLM
    else:
        logger.info("Generating final response with LLM.")
        final_response = _generate_final_response_with_llm(query, results_df)

        # å›ç­”ã‚’å±¥æ­´ã«ä¿å­˜
        if session_id:
            conv_service.add_message(
                session_id,
                "assistant",
                final_response,
                metadata={ # å¾Œã§åˆ†æã«ä½¿ãˆã‚‹ï¼ˆä¾‹: ã©ã®é¸æ‰‹ãŒã‚ˆãæ¤œç´¢ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰
                    "query_type": query_params.get("query_type"),
                    "player_name": query_params.get("name"),
                    "context_used": context_used
                }
            )
        
        # Try to enhance with chart data
        from .simple_chart_service import enhance_response_with_simple_chart
        try:
            chart_data = enhance_response_with_simple_chart(
                query, query_params, results_df, season
            )
            
            if chart_data:
                # Return response with chart data only (minimal text)
                response = {
                    "answer": "ğŸ“ˆ",  # Just chart emoji to avoid empty content message
                    "isTable": False
                }
                response.update(chart_data)
                return response
        except Exception as e:
            logger.warning(f"Chart enhancement failed: {e}")
        
        # Return regular response if no chart enhancement
        return {
            "answer": final_response,
            "isTable": False
        }


def get_ai_response_with_simple_chart(
    query: str,
    season: Optional[int] = None,
    session_id: Optional[str] = None
) -> Optional[Dict[str, Any]]:
    """æ—¢å­˜é–¢æ•°ã‚’æ‹¡å¼µã—ã¦ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒ¼ãƒˆå¯¾å¿œï¼ˆä¼šè©±å±¥æ­´å¯¾å¿œï¼‰"""

    # Just call the existing function for now - we'll integrate chart logic directly into it
    return get_ai_response_for_qna_enhanced(query, season, session_id)