from typing import Optional, List, Dict, Any
# from google.cloud import bigquery
# from google.oauth2 import service_account
from google.cloud.exceptions import GoogleCloudError
import pandas as pd
import os
import json
import requests
from dotenv import load_dotenv
# from functools import lru_cache
from datetime import datetime
from .bigquery_service import client
import logging
from backend.app.config.query_maps import ( # For Development, add backend. path
    QUERY_TYPE_CONFIG,
    METRIC_MAP,
    DECIMAL_FORMAT_COLUMNS,
    MAIN_PITCHING_STATS,
    MAIN_BATTING_STATS,
    MAIN_CAREER_BATTING_STATS,
    MAIN_RISP_BATTING_STATS, MAIN_BASES_LOADED_BATTING_STATS, MAIN_RUNNER_ON_1B_BATTING_STATS,
    MAIN_INNING_BATTING_STATS, MAIN_BATTING_BY_PITCHING_THROWS_STATS, MAIN_BATTING_BY_PITCH_TYPE_STATS,
    MAIN_BATTING_BY_GAME_SCORE_SITUATIONS_STATS
)
from backend.app.config.statcast_query import KEY_METRICS_QUERY_SELECT # For Development, add backend. path
# from .simple_chart_service import enhance_response_with_simple_chart, should_show_simple_chart # For Development, add backend. path

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.getLogger().handlers = []
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

dotenv_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', '..', '.env')
load_dotenv(dotenv_path=dotenv_path)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€
PROJECT_ID = os.getenv("GCP_PROJECT_ID")
DATASET_ID = os.getenv("BIGQUERY_DATASET_ID")
BATTING_STATS_TABLE_ID = os.getenv("BIGQUERY_BATTING_STATS_TABLE_ID", "fact_batting_stats_with_risp")
PITCHING_STATS_TABLE_ID = os.getenv("BIGQUERY_PITCHING_STATS_TABLE_ID", "fact_pitching_stats")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Manage Google cloud alient with singleton pattern
SERVICE_ACCOUNT_KEY_PATH = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")


def _parse_query_with_llm(query: str, season: Optional[int]) -> Optional[Dict[str, Any]]:
    """
    [ã‚¹ãƒ†ãƒƒãƒ—1] LLMã‚’ä½¿ã„ã€è³ªå•ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã™ã€‚ã“ã®é–¢æ•°ã¯LLMã®å½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’è§£æã—ã€ã€Œæ„å›³ã€ã‚’æ±²ã¿å–ã‚Šã€
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ¤œç´¢ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§æŠ½å‡ºã—ã¾ã™ã€‚
    """
    if not GEMINI_API_KEY:
        logger.error("GEMINI_API_KEY is not set.")
        return None

    prompt = f"""
    ã‚ãªãŸã¯MLBã®ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®"æ‰“æ’ƒæˆç¸¾ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°"ã€"æŠ•æ‰‹æˆç¸¾ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°"ã€ã¾ãŸã¯"é¸æ‰‹æˆç¸¾"ã«é–¢ã™ã‚‹ä»¥ä¸‹ã®è³ªå•ã‚’è§£æã—ã€
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ¤œç´¢ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

    # æŒ‡ç¤º
    - é¸æ‰‹åã¯è‹±èªè¡¨è¨˜ï¼ˆãƒ•ãƒ«ãƒãƒ¼ãƒ ï¼‰ã«æ­£è¦åŒ–ã—ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œå¤§è°·ã•ã‚“ã€ -> "Shohei Ohtani"
    - `season`ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‹ã‚‰å¹´ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚`season`ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€ã¾ãŸã¯ã€Œã‚­ãƒ£ãƒªã‚¢ã€ã‚„ã€Œé€šç®—ã€ãªã©ã®è¡¨ç¾ãŒã‚ã‚Œã°ã€`season`ã¯nullã«ã—ã¦ãã ã•ã„ã€‚
    - `query_type`ã¯ "season_batting"ã€"season_pitching"ã€ "batting_splits"ã€ã¾ãŸã¯ "career_batting" ã®ã„ãšã‚Œã‹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚
    - `metrics`ã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒçŸ¥ã‚ŠãŸã„æŒ‡æ¨™ã‚’ãƒªã‚¹ãƒˆå½¢å¼ã§æ ¼ç´ã—ã¦ãã ã•ã„ã€‚ä¾‹ãˆã°ã€ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³æ•°ã‚’çŸ¥ã‚ŠãŸã„å ´åˆã¯ ["homerun"] ã¨ã—ã¾ã™ã€‚æ‰“ç‡ã®å ´åˆã¯ ["batting_average"] ã¨ã—ã€å˜èªã¨å˜èªã®é–“ã«ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
    - `split_type`ã¯ã€ã€Œå¾—ç‚¹åœï¼ˆRISPï¼‰ã€ã€Œæº€å¡ã€ã€Œãƒ©ãƒ³ãƒŠãƒ¼1é¡ã€ã€Œã‚¤ãƒ‹ãƒ³ã‚°åˆ¥ã€ã€ŒæŠ•æ‰‹ãŒå·¦æŠ•ã’ã‹å³æŠ•ã’ã‹ã€ã€Œçƒç¨®åˆ¥ã€ã€Œã‚²ãƒ¼ãƒ ã‚¹ã‚³ã‚¢çŠ¶æ³åˆ¥ã€ãªã©ã®ç‰¹å®šã®çŠ¶æ³ã‚’ç¤ºã—ã¾ã™ã€‚è©²å½“ã—ãªã„å ´åˆã¯nullã«ã—ã¦ãã ã•ã„ã€‚
    - `split_type`ã§ã€game_score_situation (ã‚²ãƒ¼ãƒ ã‚¹ã‚³ã‚¢çŠ¶æ³åˆ¥) ã‚’é¸æŠã—ãŸå ´åˆã€`game_score`ã«å…·ä½“çš„ãªã‚¹ã‚³ã‚¢çŠ¶æ³ï¼ˆä¾‹ï¼š1ç‚¹ãƒªãƒ¼ãƒ‰ã€2ç‚¹ãƒ“ãƒã‚¤ãƒ³ãƒ‰ãªã©ï¼‰ã‚’ç¤ºã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
        ä¾‹ãˆã°ã€ã€Œ1ç‚¹å·®ã‚²ãƒ¼ãƒ ã€1ç‚¹ãƒªãƒ¼ãƒ‰ã€1ç‚¹ãƒ“ãƒã‚¤ãƒ³ãƒ‰ã€ã¯ã€'one_run_game'ã€'one_run_lead'ã€'one_run_trail'ã®ã‚ˆã†ã«è¡¨ç¾ã—ã¾ã™ã€‚4ç‚¹ä»¥ä¸Šã®å·®ã¯'four_plus_run_lead'ã‚„'four_plus_run_trail'ã¨ã—ã¦ãã ã•ã„ã€‚è©²å½“ã—ãªã„å ´åˆã¯nullã«ã—ã¦ãã ã•ã„ã€‚
    - `split_type`ã§ã€inning (ã‚¤ãƒ‹ãƒ³ã‚°åˆ¥) ã‚’é¸æŠã—ãŸå ´åˆã€`inning`ã«å…·ä½“çš„ãªã‚¤ãƒ‹ãƒ³ã‚°æ•°ã‚’ãƒªã‚¹ãƒˆå½¢å¼ã§ç¤ºã—ã¦ãã ã•ã„ã€‚ãƒ¬ã‚®ãƒ¥ãƒ©ãƒ¼ã‚¤ãƒ‹ãƒ³ã‚°æ•°ã¯1~9ã‚¤ãƒ‹ãƒ³ã‚°ã¾ã§ã€‚ä¾‹ï¼š1ã‚¤ãƒ‹ãƒ³ã‚°ç›®ãªã‚‰ [1]ã€7ã‚¤ãƒ‹ãƒ³ã‚°ç›®ä»¥é™ãªã‚‰ [7, 8, 9] ã¨ã—ã¾ã™ã€‚
    - `strikes`ã¯ã€ç‰¹å®šã®ã‚¹ãƒˆãƒ©ã‚¤ã‚¯æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚è©²å½“ã—ãªã„å ´åˆã¯nullã«ã—ã¦ãã ã•ã„ã€‚`balls`ã¯ã€ç‰¹å®šã®ãƒœãƒ¼ãƒ«æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚è©²å½“ã—ãªã„å ´åˆã¯nullã«ã—ã¦ãã ã•ã„ã€‚
    - ä¾‹ãˆã°ã€ã€Œãƒ•ãƒ«ã‚«ã‚¦ãƒ³ãƒˆã€ã¯ã€ `strikes`ã‚’2ã€`balls`ã‚’3ã¨ã—ã¦ãã ã•ã„ã€‚ã€Œåˆçƒã€ã¯ã€`strikes`ã‚’0ã€`balls`ã‚’0ã¨ã—ã¾ã™ã€‚è©²å½“ã—ãªã„å ´åˆã¯nullã«ã—ã¦ãã ã•ã„ã€‚
    - `pitcher_throws`ã¯ã€æŠ•æ‰‹ã®æŠ•ã’æ–¹ï¼ˆå³æŠ•ã’ã¾ãŸã¯å·¦æŠ•ã’ï¼‰ã‚’ç¤ºã—ã¾ã™ã€‚å³æŠ•ã’ã¯RHPã€å·¦æŠ•ã’ã¯LHPã¨ã—ã€è©²å½“ã—ãªã„å ´åˆã¯nullã«ã—ã¦ãã ã•ã„ã€‚
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œä¸»è¦ã‚¹ã‚¿ãƒƒãƒ„ã€ã‚„ã€Œä¸»ãªæˆç¸¾ã€ã®ã‚ˆã†ãªæ›–æ˜§ãªè¡¨ç¾ã‚’ä½¿ã£ãŸå ´åˆã€metricsã«ã¯ ["main_stats"] ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¸€ã¤ã ã‘æ ¼ç´ã—ã¦ãã ã•ã„ã€‚
    - `order_by`ã«ã¯ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®åŸºæº–ã¨ãªã‚‹æŒ‡æ¨™ã‚’ä¸€ã¤ã ã‘è¨­å®šã—ã¦ãã ã•ã„ã€‚
    - `output_format`ã§ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ "sentence" ã§ã™ã€‚ã‚‚ã—ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ã€è¡¨ã§ã€ã€ä¸€è¦§ã§ã€ã€ã¾ã¨ã‚ã¦ã€ã¨ã„ã£ãŸã‚ˆã†ãªè¨€è‘‰ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰ã€output_formatã‚’tableã«è¨­å®šã—ã¦ãã ã•ã„ã€‚ãã†ã§ãªã‘ã‚Œã°sentenceã«ã—ã¦ãã ã•ã„ã€‚

    # JSONã‚¹ã‚­ãƒ¼ãƒ
    {{
        "query_type": "season_batting" | "season_pitching" | "batting_splits" | "career_batting" | null,
        "metrics": ["string"],
        "split_type": "risp" | "bases_loaded" | "runner_on_1b" | "inning" | "pitcher_throws" | "pitch_type" | "game_score_situation" | "monthly" | null,
        "inning": ["integer"] | null,
        "strikes": "integer | null",
        "balls": "integer | null",
        "game_score": "string | null",
        "pitcher_throws": "string | null",
        "pitch_type": ["string"] | null,
        "name": "string | null",
        "season": "integer | null",
        "order_by": "string",
        "limit": "integer | null",
        "output_format": "sentence" | "table"
    }}

    # è³ªå•ã®ä¾‹
    è³ªå•: ã€Œ2023å¹´ã®ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³ç‹ã¯èª°ï¼Ÿã€
    JSON: {{ "query_type": "season_batting", "season": 2023, "metrics": ["homerun"],  "order_by": "homerun", "limit": 1 }}

    è³ªå•: ã€Œå¤§è°·ã•ã‚“ã®2024å¹´ã®RISPæ™‚ã®ä¸»è¦ã‚¹ã‚¿ãƒƒãƒ„ã¯ï¼Ÿã€
    JSON: {{ "query_type": "batting_splits", "name": "Shohei Ohtani", "season": 2024,  "metrics": ["main_stats"], "split_type": "risp", "order_by": null, "limit": 1 }}

    è³ªå•: ã€Œå¤§è°·ã•ã‚“ã®ã®2024å¹´ã®1ã‚¤ãƒ‹ãƒ³ã‚°ç›®ã®ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³æ•°ã¨OPSã‚’æ•™ãˆã¦ã€
    JSON: {{ "query_type": "batting_splits", "name": "Shohei Ohtani", "season": 2024,  "metrics": ["homerun", "on_base_plus_slugging"], "split_type": "inning", "inning": 1, "order_by": null, "limit": 1 }}

    è³ªå•: ã€Œå¤§è°·ã•ã‚“ã®2024å¹´ã®å·¦æŠ•æ‰‹ã«å¯¾ã™ã‚‹ä¸»è¦ã‚¹ã‚¿ãƒƒãƒ„ã‚’ä¸€è¦§ã§æ•™ãˆã¦ï¼Ÿã€
    JSON: {{ "query_type": "batting_splits", "name": "Shohei Ohtani", "season": 2024,  "metrics": ["main_stats"], "split_type": "pitcher_throws", "pitcher_throws": "LHP", "order_by": null, "limit": 1, "output_format": "table" }}

    è³ªå•: ã€Œå¤§è°·ã•ã‚“ã®2024å¹´ã®ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã«å¯¾ã™ã‚‹ä¸»è¦ã‚¹ã‚¿ãƒƒãƒ„ã¯ï¼Ÿã€
    JSON: {{ "query_type": "batting_splits", "name": "Shohei Ohtani", "season": 2024,  "metrics": ["main_stats"], "split_type": "pitch_type", "pitch_type": "Slider", "order_by": null }}

    è³ªå•: ã€Œå¤§è°·ã•ã‚“ã®ã‚­ãƒ£ãƒªã‚¢ä¸»è¦æ‰“æ’ƒæˆç¸¾ã‚’ä¸€è¦§ã§æ•™ãˆã¦ã€
    JSON: {{ "query_type": "career_batting", "name": "Shohei Ohtani", "metrics": ["main_stats"], "order_by": null, "limit": 1, "output_format": "table" }}

    è³ªå•: ã€Œå¤§è°·ã•ã‚“ã®2024å¹´ã®ã€1ç‚¹ãƒ“ãƒã‚¤ãƒ³ãƒ‰ã§ã®ä¸»è¦ã‚¹ã‚¿ãƒƒãƒ„ã¯ï¼Ÿã€
    JSON: {{ "query_type": "batting_splits", "name": "Shohei Ohtani", "season": 2024,  "metrics": ["main_stats"], "split_type": "game_score_situation", "game_score": "one_run_trail", "order_by": null, "limit": 1 }}

    è³ªå•: ã€Œå¤§è°·ã•ã‚“ã®2024å¹´ã®æ‰“ç‡ã‚’æœˆæ¯ã®æ¨ç§»ã‚’ãƒãƒ£ãƒ¼ãƒˆã§æ•™ãˆã¦ã€
    JSON: {{ "query_type": "batting_splits", "name": "Shohei Ohtani", "season": 2024,  "metrics": ["batting_average"], "split_type": "monthly", "order_by": null }}

    # è¤‡åˆè³ªå•ã®ä¾‹
    è³ªå•: ã€Œå¤§è°·ã•ã‚“ã®2024å¹´ã®7ã‚¤ãƒ‹ãƒ³ã‚°ç›®ä»¥é™ã€ãƒ•ãƒ«ã‚«ã‚¦ãƒ³ãƒˆã§ã®ã€RISPæ™‚ã®ä¸»è¦ã‚¹ã‚¿ãƒƒãƒ„ã¯ï¼Ÿã€
    JSON: {{ "query_type": "batting_splits", "name": "Shohei Ohtani", "season": 2024,  "metrics": ["main_stats"], "split_type": "risp", "inning": [7, 8, 9], "strikes": 2, "balls": 3, "order_by": null, "limit": 1 }}

    # æœ¬ç•ª
    è³ªå•: ã€Œ{query}ã€
    JSON:
    """

    GEMINI_API_URL=f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}"
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"responseMimeType": "application/json"}
    }

    try:
        response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(payload))
        response.raise_for_status()
        result = response.json()
        if result.get("candidates"):
            json_string = result["candidates"][0]["content"]["parts"][0]["text"]
            params = json.loads(json_string)

            logger.info(f"Parsed parameters: {params}")

            if season and 'season' not in params:
                params['season'] = season
            return params
        return None
    except (requests.exceptions.RequestException, json.JSONDecodeError) as e:
        logger.error(f"Error during LLM query parsing: {e}", exc_info=True)
        return None


# Helper function to determine query strategy (using a simple query or more complex one)
def _determine_query_strategy(params: Dict[str, Any]) -> str:
    """
    ã‚¯ã‚¨ãƒªã®è¤‡é›‘ã•ã«åŸºã¥ã„ã¦æˆ¦ç•¥ã‚’æ±ºå®šã™ã‚‹
    - è¤‡åˆæ¡ä»¶ãŒ2ã¤ä»¥ä¸Š: statcast master table
    - å˜ä¸€æ¡ä»¶: aggregated table
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ãŒå¿…è¦ãªå ´åˆã®ç‰¹åˆ¥å‡¦ç†ã‚‚å«ã‚€
    """

    complex_conditions = []

    # ã‚¤ãƒ‹ãƒ³ã‚°æ¡ä»¶
    if params.get("inning"):
        complex_conditions.append("inning")
    
    # ã‚«ã‚¦ãƒ³ãƒˆæ¡ä»¶
    if params.get("strikes") is not None:
        complex_conditions.append("strikes")
    if params.get("balls") is not None:
        complex_conditions.append("balls")
    
    # æŠ•æ‰‹ã‚¿ã‚¤ãƒ—æ¡ä»¶
    if params.get("pitcher_throws"):
        complex_conditions.append("pitcher_throws")
    
    # çƒç¨®æ¡ä»¶
    if params.get("pitch_type"):
        complex_conditions.append("pitch_type")
    
    # çŠ¶æ³æ¡ä»¶
    situational_splits = ["risp", "bases_loaded", "runner_on_1b"]
    if params.get("split_type") in situational_splits:
        complex_conditions.append("situational")
    
    # ã‚²ãƒ¼ãƒ ã‚¹ã‚³ã‚¢çŠ¶æ³æ¡ä»¶
    if params.get("game_score"):
        complex_conditions.append("game_score")
    
    # è¤‡åˆæ¡ä»¶ã®åˆ¤å®š
    condition_count = len(complex_conditions)

    # ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹ï¼šè¤‡æ•°å¹´ãƒ‡ãƒ¼ã‚¿ + è¤‡åˆæ¡ä»¶ã¯é‡ã™ãã‚‹å¯èƒ½æ€§
    if not params.get("season") and condition_count >= 2:
        logger.warning(f"Multi-year query with {condition_count} complex conditions may be slow")

    strategy = "statcast_master_table" if condition_count >= 2 else "aggregated_table"

    logger.info(f"Query strategy: {strategy} based on condition count: {condition_count}")
    return strategy


# Helper function to build dynamic SQL queries with statcast_master_table
def _build_dynamic_statcast_sql(params: Dict[str, Any]) -> str:
    """
    Build a SQL query for the statcast_master_table based on the provided parameters.
    """

    metrics = params.get("metrics", [])
    if not metrics:
        return None
    
    # Replace keyword from "main_stats" with related column list
    if metrics == ["main_stats"]:
        metrics = MAIN_BATTING_STATS # tentative
    
    table_name = "tbl_statcast_2021_2025_master"
    year_column = "game_year"
    player_name_col = "batter_name"

    
    # static query part
    # SELECT clause
    # if all seasons
    if not params.get("season"):
        select_clause = KEY_METRICS_QUERY_SELECT
    else:
        select_clause = KEY_METRICS_QUERY_SELECT + ", game_year"

    # dynamic query part
    # WHERE clause
    where_condition = []
    if params.get("name"):
        where_condition.append(f"{player_name_col} = '{params['name']}'")
    if params.get("season"):
        where_condition.append(f"{year_column} = {params['season']}")
    if params.get("inning"):
        # Ensure inning is a list of integers, then join as comma-separated string
        innings = params['inning']
        if isinstance(innings, list):
            inning_list = ", ".join(str(int(i)) for i in innings)
            where_condition.append(f"inning IN ({inning_list})")
        else:
            where_condition.append(f"inning = {int(innings)}")
    if params.get('pitch_throws'):
        where_condition.append(f"p_throws = {params['pitch_throws']}")
    if params.get("strikes"):
        where_condition.append(f"strikes = {params['strikes']}")
    if params.get("balls"):
        where_condition.append(f"balls = {params['balls']}")
    if params.get('split_type'):
        if params['split_type'] == 'risp':
            where_condition.append("(on_2b != 0 OR on_3b != 0)")
        if params['split_type'] == 'bases_loaded':
            where_condition.append("(on_1b != 0 AND on_2b != 0 AND on_3b != 0)")
        if params['split_type'] == 'runner_on_1b':
            where_condition.append("(on_1b != 0 AND on_2b = 0 AND on_3b = 0)")
    # if params.get('game_score'): # At this point, data source is not accurate due to incorrect logic. To be fixed.
    #     pass

    where_clause = f"WHERE events IS NOT NULL AND game_type = 'R' AND {' AND '.join(where_condition)}" if where_condition else ""

    # GROUP BY clause # all seasons can be selected, to be updated
    if params.get("season"):
        group_by_clause = f"GROUP BY {year_column}, {player_name_col}"
    else:
        group_by_clause = f"GROUP BY {player_name_col}"

    # ORDER BY clause # To be implmented later
    order_by_clause = ""
    # if params.get("order_by"):
    #     order_by_clause = f"ORDER BY {params['order_by']}"
    #     order_direction = "ASC" if order_by_col in ("era", "whip", "fip") else "DESC"
    #     order_by_clause = f"ORDER BY {order_by_col} {order_direction}"

    # LIMIT clause
    if params.get("limit") is not None:
        limit = params.get("limit", 10)
        limit_clause = f"LIMIT {limit}"
    else:
        limit_clause = ""
    
    return f"{select_clause} FROM `{PROJECT_ID}.{DATASET_ID}.{table_name}` {where_clause} {group_by_clause} {order_by_clause} {limit_clause}"
        
 

# Helper function to build dynamic SQL queries with aggregated table
def _build_dynamic_sql(params: Dict[str, Any]) -> str:
    """
    [ã‚¹ãƒ†ãƒƒãƒ—2] æŠ½å‡ºã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…ƒã«ã€BigQueryç”¨ã®SQLã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
    """

    # Without query_type and metrics, a query can not be constructed
    query_type = params.get("query_type", [])
    metrics = params.get("metrics", []) # multiple metrics could be stored in the dictionary
    if not query_type or not metrics:
        return None
    
    split_type = params.get("split_type", [])
    
    # Replace keyword from "main_stats" with related column list
    if metrics == ["main_stats"]:
        if query_type == "season_pitching":
            metrics = MAIN_PITCHING_STATS
        elif query_type == "season_batting":
            metrics = MAIN_BATTING_STATS
        elif query_type == "career_batting":
            metrics = MAIN_CAREER_BATTING_STATS
        elif query_type == "batting_splits" and split_type == "risp":
            metrics = MAIN_RISP_BATTING_STATS
        elif query_type == "batting_splits" and split_type == "bases_loaded":
            metrics = MAIN_BASES_LOADED_BATTING_STATS
        elif query_type == "batting_splits" and split_type == "runner_on_1b":
            metrics = MAIN_RUNNER_ON_1B_BATTING_STATS
        elif query_type == "batting_splits" and split_type == "inning":
            metrics = MAIN_INNING_BATTING_STATS
        elif query_type == "batting_splits" and split_type == "pitcher_throws":
            metrics = MAIN_BATTING_BY_PITCHING_THROWS_STATS
        elif query_type == "batting_splits" and split_type == "pitch_type":
            metrics = MAIN_BATTING_BY_PITCH_TYPE_STATS
        elif query_type == "batting_splits" and split_type == "game_score_situation":
            metrics = MAIN_BATTING_BY_GAME_SCORE_SITUATIONS_STATS
        # Add another metrics if needed from here

    # Initialize variables
    config = None
    metric_map_key_base = query_type  # Default key for METRIC_MAP

    # Get config info
    if query_type in ["season_batting", "season_pitching", "career_batting"]:
        config = QUERY_TYPE_CONFIG.get(query_type)
    elif query_type == "batting_splits" and params.get("split_type"):
        split_type = params.get("split_type")
        config = QUERY_TYPE_CONFIG.get(query_type, {}).get(split_type)
        metric_map_key_base = f"{query_type}_{split_type}"

    if not config:
        logger.error(f"Configuration not found for query_type: {query_type}")
        return None
    
    table_name = config["table_id"]
    year_column = config["year_col"]
    month_column = config.get("month_col", None)
    player_name_col = config["player_col"]
    # default_colsã‚’å‹•çš„ã«è¨­å®š
    if query_type == "career_batting":
        default_cols = [f"{player_name_col} as name", "career_last_team"]
    elif split_type == "monthly" and month_column:
        default_cols = [f"{player_name_col} as name", f"{month_column} as month"]
    else:
        default_cols = [f"{player_name_col} as name", f"{year_column} as season"]
    if "team" in config.get("available_metrics", []) or config.get("table_id") in [BATTING_STATS_TABLE_ID, PITCHING_STATS_TABLE_ID]:
        if "team" not in default_cols:
            default_cols.insert(1, "team")

    # METRIC_MAPã‹ã‚‰å®‰å…¨ã«å€¤ã‚’å–å¾—ã—ã€Noneã‚’é™¤å¤–
    if query_type == "career_batting":
        queried_metrics = []
        # Group by context first (career, risp, bases_loaded), then by metric type
        for key in ["career", "risp", "bases_loaded"]:
            for metric in metrics:
                metric_mapping = METRIC_MAP.get(metric, {}).get(metric_map_key_base, {}).get(key)
                if metric_mapping:
                    queried_metrics.append(metric_mapping)
    else:
        queried_metrics = [METRIC_MAP.get(metric, {}).get(metric_map_key_base) for metric in metrics]
    # Debugging
    logger.debug(f"Queried metrics for {query_type}: {queried_metrics}")
    # NoneãŒãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ã¨SQLæ–‡æ³•ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ãŸã‚ã€ã“ã“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    valid_queried_metrics = [m for m in queried_metrics if m is not None]

    # SELECT clause
    if split_type == "pitch_type":
        select_cols = default_cols + ["pitch_name"] + [m for m in valid_queried_metrics if m not in ['name', 'team', 'season']]
    else:
        select_cols = default_cols + [m for m in valid_queried_metrics if m not in ['name', 'team', 'season']]
    # Deduplicate
    final_select_cols = list(dict.fromkeys(select_cols))
    select_clause = f"SELECT {', '.join(final_select_cols)}"

    # WHERE clause
    where_condition = []
    if params.get("name"):
        where_condition.append(f"{player_name_col} = '{params['name']}'")
    if params.get("season"):
        where_condition.append(f"{year_column} = {params['season']}")
    if params.get("inning") is not None and split_type == "inning": # condition by inning
        where_condition.append(f"inning = {params['inning']}")
    if params.get("pitcher_throws") and split_type == "pitcher_throws":  # condition by pitcher throws
        where_condition.append(f"p_throws = '{params['pitcher_throws']}'")
    if params.get("pitch_type") and split_type == "pitch_type":  # condition by pitch type
        # if multiple pitch types are provided
        if len(params['pitch_type']) > 1:
            where_condition.append(
                "pitch_name IN ({})".format(
                    ", ".join("'{}'".format(pt) for pt in params['pitch_type'])
                )
            )
        else:
            # ãƒªã‚¹ãƒˆã®æœ€åˆã®è¦ç´ ã‚’å–å¾—
            where_condition.append(f"pitch_name = '{params['pitch_type'][0]}'")
    
    # At this point, data source is not accurate due to incorrect logic. To be fixed.
    # if params.get("game_score") and split_type == "game_score_situation":  # condition by game score situation
    #     mapping = {
    #         'one_run_game':      "game_score_situation IN ('1-run lead', '1-run trail')",
    #         'one_run_lead':      "game_score_situation = '1-run lead'",
    #         'one_run_trail':     "game_score_situation = '1-run trail'",
    #         'two_run_game':      "game_score_situation IN ('2-run lead', '2-run trail')",
    #         'two_run_lead':      "game_score_situation = '2-run lead'",
    #         'two_run_trail':     "game_score_situation = '2-run trail'",
    #         'three_run_game':    "game_score_situation IN ('3-run lead', '3-run trail')",
    #         'three_run_lead':    "game_score_situation = '3-run lead'",
    #         'three_run_trail':   "game_score_situation = '3-run trail'",
    #         'four_plus_run_game':"game_score_situation IN ('4+ run lead', '4+ run trail')",
    #         'four_plus_run_lead':"game_score_situation = '4+ run lead'",
    #         'four_plus_run_trail':"game_score_situation = '4+ run trail'",
    #         'tie_game':          "game_score_situation = 'Tie game'",
    #     }
    #     condition = mapping.get(params['game_score'])
    #     if condition:
    #         where_condition.append(condition)
    #     else:
    #         logger.warning(f"Unknown game_score parameter: {params['game_score']}")


    where_clause = f"WHERE {' AND '.join(where_condition)}" if where_condition else ""

    # GROUP BY clause
    group_by_clause = ""
    if params.get("pitch_type") and split_type == "pitch_type" and len(params['pitch_type']) > 1:
        group_by_clause = f"GROUP BY {', '.join([player_name_col, year_column, 'pitch_name'] + [m for m in valid_queried_metrics if m not in ['name', 'team', 'season']])}"

    # ORDER BY clause
    order_by_clause = ""
    if params.get("order_by"):
        order_by_col = METRIC_MAP.get(params["order_by"], {}).get(metric_map_key_base, params["order_by"])
        order_direction = "ASC" if order_by_col in ("era", "whip", "fip") else "DESC"
        order_by_clause = f"ORDER BY {order_by_col} {order_direction}"
    elif split_type == "monthly" and month_column:
        order_by_clause = f"ORDER BY {month_column} ASC"

    # LIMIT clause
    if params.get("limit") is not None:
        limit = params.get("limit", 10)
        limit_clause = f"LIMIT {limit}"
    else:
        limit_clause = ""

    return f"{select_clause} FROM `{PROJECT_ID}.{DATASET_ID}.{table_name}` {where_clause} {group_by_clause} {order_by_clause} {limit_clause}"


def _generate_final_response_with_llm(original_query: str, data_df: pd.DataFrame) -> str:
    """
    [ã‚¹ãƒ†ãƒƒãƒ—4] å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã¨å…ƒã®è³ªå•ã«åŸºã¥ã„ã¦ã€LLMãŒè‡ªç„¶è¨€èªã®å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    * ã‚¹ãƒ†ãƒƒãƒ—3ã¯BigQueryã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã“ã¨ã§ã™ã€‚
    """
    if not GEMINI_API_KEY:
        logger.error("GEMINI_API_KEY is not set.")
        return "AIã¨ã®é€šä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    data_json_str = data_df.to_json(orient='records', indent=2, force_ascii=False)
    prompt = f"""
    ã‚ãªãŸã¯MLBã®ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç°¡æ½”ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
    ãƒ‡ãƒ¼ã‚¿ã¯è¡¨å½¢å¼ã§æç¤ºã™ã‚‹ã®ã§ã¯ãªãã€è‡ªç„¶ãªæ–‡ç« ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

    ---
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {original_query}
    æä¾›ãƒ‡ãƒ¼ã‚¿ (JSONå½¢å¼):
    {data_json_str}
    ---
    å›ç­”:
    """
    GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}"
    headers = {"Content-Type": "application/json"}
    payload = {"contents": [{"parts": [{"text": prompt}]}]}

    try:
        response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(payload))
        response.raise_for_status()
        result = response.json()
        if result.get("candidates"):
            generated_text = result["candidates"][0]["content"]["parts"][0]["text"]
            return generated_text.replace('\n', '<br>')
        return "AIã«ã‚ˆã‚‹å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    except requests.exceptions.RequestException as e:
        logger.error(f"Error calling Gemini API for final response: {e}", exc_info=True)
        return "AIã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"


def get_ai_response_for_qna_enhanced(query: str, season: Optional[int] = None) -> Optional[Dict[str, Any]]:
    """
    ã€æ‰“æ’ƒãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ç‰¹åŒ–ç‰ˆã€‘
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®"æ‰“æ’ƒãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰"ã«é–¢ã™ã‚‹è³ªå•ã‚’å‡¦ç†ã—ã¾ã™ã€‚
    """
    # Step 1: LLMã§è³ªå•ã‚’è§£æ
    query_params = _parse_query_with_llm(query, season)
    if not query_params:
        logger.warning("Could not extract parameters from the query.")
        return {
            "answer": "è³ªå•ã‚’ç†è§£ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ‰“æ’ƒæˆç¸¾ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„ã€‚ï¼ˆä¾‹ï¼š2024å¹´ã®ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³ç‹ã¯èª°ï¼Ÿï¼‰",
            "isTable": False
        }
    logger.info(f"Parsed query parameters: {query_params}")

    # Step 2: Build SQL
    query_strategy = _determine_query_strategy(query_params)
    logger.info(f"Using query strategy: {query_strategy}")

    if query_strategy == "aggregated_table":
        # Using aggregated table
        sql_query = _build_dynamic_sql(query_params)
        if not sql_query:
            logger.warning("Failed to build SQL query.")
            return {
                "answer": "ã“ã®è³ªå•ã«å¯¾å¿œã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
                "isTable": False
            }
        logger.info(f"Generated SQL query:\n{sql_query}")
    
    else: # Using statcast master table
        sql_query = _build_dynamic_statcast_sql(query_params)
        if not sql_query:
            logger.warning("Failed to build SQL query with statcast master table.")
            return {
                "answer": "ã“ã®è³ªå•ã«å¯¾å¿œã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
                "isTable": False
            }
        logger.info(f"Generated SQL query (strategy: {query_strategy}):\n{sql_query}")

    # Step 3: Fetch data from BigQuery
    try:
        query_start = datetime.now()
        results_df = client.query(sql_query).to_dataframe()
        query_duration = (datetime.now() - query_start).total_seconds()

        logger.info(f"Query completed in {query_duration:.2f}s, fetched {len(results_df)} rows")

        # Performance warning for slow queries
        if query_duration > 10:  # 10ç§’ä»¥ä¸Š
            logger.warning(f"Slow query detected: {query_duration:.2f}s")
    except GoogleCloudError as e:
        logger.error(f"BigQuery query failed: {e}", exc_info=True)

        # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        error_message = "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
        if "timeout" in str(e).lower():
            error_message += "ã‚¯ã‚¨ãƒªãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚æ¡ä»¶ã‚’çµã£ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
        elif "quota" in str(e).lower():
            error_message += "åˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
        
        return {
            "answer": error_message,
            "isTable": False
        }
    
    if results_df.empty:
        return {
            "answer": "æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’å¤‰æ›´ã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚",
            "isTable": False
        }
    
    # Step 4: Format response
    total_duration = (datetime.now() - query_start).total_seconds()
    logger.info(f"Total request processing time: {total_duration:.2f}s")
    
    # if output format is table
    if query_params.get("output_format") == "table":
        # Debug logging
        logger.info(f"DataFrame columns: {results_df.columns.tolist()}")
        logger.info(f"DataFrame dtypes: {results_df.dtypes.to_dict()}")
        logger.info(f"First row sample: {results_df.iloc[0].to_dict() if len(results_df) > 0 else 'No data'}")
        
        # Use centralized decimal columns configuration
        decimal_columns = DECIMAL_FORMAT_COLUMNS
        
        # Force decimal columns to have proper numeric types BEFORE converting to dict
        for col in decimal_columns:
            if col in results_df.columns:
                # Convert to numeric, coercing errors to NaN, then fill NaN with None
                results_df[col] = pd.to_numeric(results_df[col], errors='coerce')
                results_df[col] = results_df[col].where(pd.notnull(results_df[col]), None)
        
        # Convert to dictionary
        table_data = results_df.to_dict('records')
        
        # Post-process to ensure decimal formatting
        for row in table_data:
            for col in decimal_columns:
                if col in row and row[col] is not None:
                    try:
                        # Ensure it's a proper decimal number
                        value = float(row[col])
                        if not pd.isna(value):
                            row[col] = round(value, 3)
                        else:
                            row[col] = None
                    except (ValueError, TypeError) as e:
                        logger.warning(f"Could not convert {col} value {row[col]} to float: {e}")
                        # Keep original value
                        pass
        
        # Debug the final table data
        logger.info(f"Final table_data sample: {table_data[0] if table_data else 'No data'}")
        
        columns = [{"key": col, "label": col.replace('_', ' ').title()} for col in results_df.columns]
        
        # Check if single row result for transposition
        is_single_row = len(results_df) == 1
        
        # Add grouping metadata for career batting
        grouping_info = None
        if query_params.get("query_type") == "career_batting":
            # Get base info columns (name, team, etc.)
            base_columns = [col for col in results_df.columns if col in ['name', 'batter_name', 'career_last_team']]
            career_base_columns = [col for col in results_df.columns if col.startswith('career_') and '_at_' not in col and '_by_' not in col]
            risp_columns = [col for col in results_df.columns if '_at_risp' in col]
            bases_loaded_columns = [col for col in results_df.columns if '_at_bases_loaded' in col]
            
            grouping_info = {
                "type": "career_batting_chunks",
                "groups": [
                    {
                        "name": "Career Stats",
                        "columns": base_columns + career_base_columns
                    },
                    {
                        "name": "Career RISP Stats", 
                        "columns": risp_columns
                    },
                    {
                        "name": "Career Bases Loaded Stats",
                        "columns": bases_loaded_columns
                    }
                ]
            }
        
        return {
            "answer": f"ä»¥ä¸‹ã¯{len(results_df)}ä»¶ã®çµæœã§ã™ï¼š",
            "isTable": True,
            "isTransposed": is_single_row,
            "tableData": table_data,
            "columns": columns,
            "decimalColumns": [col for col in results_df.columns if col in DECIMAL_FORMAT_COLUMNS],
            "grouping": grouping_info
        }

    # Step 4: Generate final response with LLM
    else:
        logger.info("Generating final response with LLM.")
        final_response = _generate_final_response_with_llm(query, results_df)
        
        # Try to enhance with chart data
        from .simple_chart_service import enhance_response_with_simple_chart
        try:
            chart_data = enhance_response_with_simple_chart(
                query, query_params, results_df, season
            )
            
            if chart_data:
                # Return response with chart data only (minimal text)
                response = {
                    "answer": "ğŸ“ˆ",  # Just chart emoji to avoid empty content message
                    "isTable": False
                }
                response.update(chart_data)
                return response
        except Exception as e:
            logger.warning(f"Chart enhancement failed: {e}")
        
        # Return regular response if no chart enhancement
        return {
            "answer": final_response,
            "isTable": False
        }


def get_ai_response_with_simple_chart(query: str, season: Optional[int] = None) -> Optional[Dict[str, Any]]:
    """æ—¢å­˜é–¢æ•°ã‚’æ‹¡å¼µã—ã¦ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒ¼ãƒˆå¯¾å¿œ"""
    
    # Just call the existing function for now - we'll integrate chart logic directly into it
    return get_ai_response_for_qna_enhanced(query, season)